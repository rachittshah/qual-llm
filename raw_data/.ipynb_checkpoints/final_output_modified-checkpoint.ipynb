{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f457316e",
   "metadata": {},
   "source": [
    "## AI workshop: Rachitt Shah\n",
    "\n",
    "### Data cleaning and preprocessing:\n",
    "\n",
    "The JSONs had different schemas and for the LLMs to make sense of them, I've converted them into a CSV. Some known issues with the data are: \n",
    "\n",
    "- responses json had an invalid key for 18_1, for music type, which didn't have a question.\n",
    "- Questions JSON had a question for users to understand the spacing out of episode airing frequencies, which didn't have a key in the responses JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4c8a4",
   "metadata": {},
   "source": [
    "## Approach One:\n",
    "\n",
    "My first approach is feeding the CSVs directly to the agents. This is ultimately the end process for al, but this was the fastest to demo.\n",
    "\n",
    "There's two approaches to this:\n",
    "\n",
    "- Use the Chat models: these models worked the best with more qualatative data, but they're also the most expensive yet best results. Prone to missing actions, and need to be prompted well. Tested models: gpt-3.5-turbo-16k(to save time on chunking) and GPT-4. GPT-4 is the best, but it's also the most expensive. GPT-3.5 is the best for the price, but it's still expensive.\n",
    "\n",
    "- Instruct Models: text-davinci-003 performed the best on the quantative data, since it's an instruct-tuned model. This model performed best on straightforward asks, such as finding gender splits. Known constrait is token size and hallucinations when scaling to more complex prompts.\n",
    "\n",
    "A mixed approach would be prompt selectors by langchain, which can invoke the best model for the job. Unfortunately, there's no docs for this, but this is a viable plan for prod scale deployments and reducing costs.\n",
    "\n",
    "My agent type is zero shot, since future data is unknown, and having scalable prompts and agent pipelines is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3b816-8d4a-45c8-be30-e5bde1e8c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "import pandas as pd\n",
    "import os\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce771325",
   "metadata": {},
   "source": [
    "### Chat template improvement\n",
    "\n",
    "for a prod scale system, using CustomChatTemplate from Langchain is the best approach to take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d53be-aeb3-49d5-9e70-262205d94db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]= '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'\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"]=\"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\"\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# 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\n",
    "\n",
    "prompt = \"\"\"\n",
    "### Instructions:\n",
    "\n",
    "As a User Researcher, your expertise lies in analyzing datasets for user research. Your task is to transform a user researcher's question into a Python code snippet using pandas. This code snippet should be designed to extract insights from the provided datasets using a blend of qualitative and quantitative methods.\n",
    "\n",
    "**Important Guidelines**:\n",
    "\n",
    "- **Contextual Understanding**: Recognize the context of each question and determine which dataset it pertains to. Choose the appropriate dataset for extraction.\n",
    "- **Clarity and Thoroughness**: Ensure clarity in the code. Make use of clear variable names, and break down the code into smaller chunks if necessary for better readability. Deliver thorough, actionable insights with every interaction.\n",
    "- **Annotations and Comments**: Each code snippet should have comments explaining the steps being taken, so you can understand and potentially modify it in the future.\n",
    "- **Visual Representations**: When visual clarity is beneficial or suggested by the question, produce graphs, plots, and histograms using libraries like `matplotlib` or `seaborn`.\n",
    "- **Optimization**: Ensure that the code is optimized for performance, especially when dealing with large datasets.\n",
    "\n",
    "### Input:\n",
    "Translate the user researcher's question: {input} to extract the desired information from the datasets.\n",
    "\n",
    "### Response:\n",
    "Your response should include graphs, plots, and histograms, as well as a summary of your results. Exclude code outputs, and give a comprehensive understanding of youtr results as a User Researcher.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74429b",
   "metadata": {},
   "source": [
    "### Toolkits\n",
    "\n",
    "Why have i gone for pandas agents instead of using llm-math?\n",
    "\n",
    "- LLM-Math is not optimised for large datasets and performing complex operations which pandas can handle easily.\n",
    "- If LLM-Math has to be implemented, I'd recommend having a list of quantatative operation functions created and invoked by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343211b-0d56-49de-83d7-a7c457c11e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/home/rachitt/qual-llm/raw_data/survey_1.csv')\n",
    "df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_1_instruct = create_pandas_dataframe_agent(OpenAI(model=\"text-davinci-003\", temperature=0.3), df_1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48121f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_1_instruct.run(f\"{prompt} what is the average age of the respondents? Show as a graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a394d4-9768-4e87-9899-4c46010afd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_agent = create_pandas_dataframe_agent(ChatOpenAI(model=\"gpt-4-0613\",temperature=0.3), df_1, verbose=True,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2679d-91f5-499f-bb34-e8ac64fc6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_string = pd_agent.run(f\"{prompt} How many respondents prefer to watch alone vs. with friends/family, show a graph too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d25c88-b97f-4811-8f2b-faca569e9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('/home/rachitt/qual-llm/raw_data/survey_2.csv')\n",
    "df_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65c4c0-92c0-4331-bdf2-9d4e6fda8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_instruct = create_pandas_dataframe_agent(OpenAI(model=\"text-davinci-003\", temperature=0.3), df_2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d15067-cac6-4c05-b728-df47cd559cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_instruct.run(f\"{prompt}Is Subway the most popular sandwich shop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78a341-efa2-418c-9fc5-e6969a0d4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_agent = create_pandas_dataframe_agent(ChatOpenAI(model=\"gpt-4-0613\",temperature=0.5), df_2, verbose=True, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_agent.run(f\"{prompt} What is the gender split among folks who ranked Taco bell as their top choice? Plot a piechart to show this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efdcff",
   "metadata": {},
   "source": [
    "### Mutliple CSV agents\n",
    "\n",
    "A brute force way to index CSVs, which can be used for faster A/B testing. Not a recommended way, since we're using zero shot learning, due to which the model will lose context and produce lower quality results.\n",
    "\n",
    "There's also a known issue with multiple file agents is that for CSV agents, the OutputParser class from Langchain throws issues alot of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331e79e-0431-4df3-8864-ef96146b138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "Multi_DF_agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    [df_1,df_2],\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb46785-d925-4bbe-8095-0f46dd4d96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_DF_agent.run(f\"{prompt} What is the gender split among folks who ranked Taco bell as their top choice? Plot a piechart to show this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc2efc-68d7-4390-b015-d8298f8b3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent, create_csv_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent = create_csv_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    [\"/home/rachitt/qual-llm/raw_data/survey_2.csv\", \"/home/rachitt/qual-llm/raw_data/survey_1.csv\"],\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7368be-86d9-445a-9018-d4da73a04c74",
   "metadata": {},
   "source": [
    "## Running Agents with monitoring\n",
    "\n",
    "Running all of our function calls with [https://www.trulens.org/], which allows evals+feedback for our LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6eb876-24fa-456d-80c2-9a2c9a9a93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Huggingface-based feedback function collection class:\n",
    "hugs = Huggingface()\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275bd67-f1a1-497b-80cf-e19af237dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_instruct_survey_1 = TruChain(pd_1_instruct, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],                                  \n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\n",
    "\n",
    "llm_response = truchain_instruct_survey_1(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dc180-4a7b-4412-adb6-c634678ca7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_chat_survey_1 = TruChain(pd_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What is the gender split between people who use pay for the streaming service themselves vs share an account with someone?\"\n",
    "\n",
    "llm_response = truchain_chat_survey_1(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a164514-20fd-487c-ada3-04656eb80efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_instruct_survey_2 = TruChain(pd_2_instruct, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What is the gender split in the survey?\"\n",
    "\n",
    "llm_response = truchain_instruct_survey_2(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ce03b-e863-4271-aad8-fcc7cd14a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_chat_survey_2 = TruChain(pd_2_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} Is Subway the most popular sandwich shop?\"\n",
    "\n",
    "llm_response = truchain_chat_survey_2(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad06416-1af3-4bcb-88bd-30cb11a83e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_multiple_df = TruChain(Multi_DF_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the top three highest ranked mexican fast food restaurants? Plot a chart.?\"\n",
    "\n",
    "llm_response = truchain_multiple_df(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6fd75-b66e-4148-b9d5-96ef769c8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975ea65-28d4-439b-a2e8-28c863ef30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53e175-8392-473e-9485-46790903b760",
   "metadata": {},
   "source": [
    "### Vectorstore+Agents approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f0c0e-05cd-44d3-96ad-f65b616ba128",
   "metadata": {},
   "source": [
    "Vectorstores can significantly increase the context of the models, with a larger size.\n",
    "\n",
    "I wished to benchmark each approach to understand which approach performed the best on question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d408e-0f8d-486b-8810-cf01201bd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "loader = DirectoryLoader(\"/home/rachitt/qual-llm/raw_data\", glob=\"**/*.csv\",use_multithreading=True, loader_cls=CSVLoader, show_progress=True)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"survey_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c21e-572d-431d-887b-e75449524d12",
   "metadata": {},
   "source": [
    "agents should have been of type OpenAI functions for better compatibility with toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51fa5d-f1cb-45dd-9cba-67ad9f31daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def checker_tool_function(text: str) -> str:\n",
    "    from langchain.chains import LLMCheckerChain\n",
    "    from langchain.llms import OpenAI\n",
    "\n",
    "    # Initialize the LLM (Language Model)\n",
    "    llm = OpenAI(temperature=0.3)\n",
    "\n",
    "    # Initialize the LLMCheckerChain\n",
    "    checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "\n",
    "    # Run the chain with the given text\n",
    "    return checker_chain.run(text)\n",
    "\n",
    "pandas_tool = [\n",
    "    Tool(\n",
    "        name='Pandas Data frame tool',\n",
    "        func=pd_agent.run,\n",
    "        description=\"Useful for when you need to answer questions about survey_1 about data on media consumption\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Pandas Data frame tool\",\n",
    "        func=pd_2_agent.run,\n",
    "        description=\"useful for when you need to answer questions about survey_2 about data on eating habits.\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]\n",
    "\n",
    "checker_tool = Tool.from_function(\n",
    "    func=checker_tool_function,\n",
    "    name=\"LLMChecker\",\n",
    "    description=\"Checks and validates the response from LLM\"\n",
    ")\n",
    "\n",
    "# Initialize the tools list and then append the tools to it\n",
    "tools = []\n",
    "tools.extend(pandas_tool)\n",
    "tools.append(checker_tool)\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31543182-be3d-40ec-96ab-696e2bebdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_qa_vectordb = TruChain(conversational_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\n",
    "\n",
    "llm_response = truchain_qa_vectordb(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a2b2d-3012-47d6-ad18-0b0fc73ef422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pd_agent.agent.llm_chain.prompt.template) \n",
    "\n",
    "# sys_message = \"YOUR NEW PROMPT IS HERE\"\n",
    "# zero_shot_agent.agent.llm_chain.prompt.template = sys_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc108902-5263-4b33-b246-e9c2a9082e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
