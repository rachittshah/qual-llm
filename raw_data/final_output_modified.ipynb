{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f457316e",
   "metadata": {},
   "source": [
    "## AI workshop: Rachitt Shah\n",
    "\n",
    "### Data cleaning and preprocessing:\n",
    "\n",
    "The JSONs had different schemas and for the LLMs to make sense of them, I've converted them into a CSV. Some known issues with the data are: \n",
    "\n",
    "- responses json had an invalid key for 18_1, for music type, which didn't have a question.\n",
    "- Questions JSON had a question for users to understand the spacing out of episode airing frequencies, which didn't have a key in the responses JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4c8a4",
   "metadata": {},
   "source": [
    "## Approach One:\n",
    "\n",
    "My first approach is feeding the CSVs directly to the agents. This is ultimately the end process for al, but this was the fastest to demo.\n",
    "\n",
    "There's two approaches to this:\n",
    "\n",
    "- Use the Chat models: these models worked the best with more qualatative data, but they're also the most expensive yet best results. Prone to missing actions, and need to be prompted well. Tested models: gpt-3.5-turbo-16k(to save time on chunking) and GPT-4. GPT-4 is the best, but it's also the most expensive. GPT-3.5 is the best for the price, but it's still expensive.\n",
    "\n",
    "- Instruct Models: text-davinci-003 performed the best on the quantative data, since it's an instruct-tuned model. This model performed best on straightforward asks, such as finding gender splits. Known constrait is token size and hallucinations when scaling to more complex prompts.\n",
    "\n",
    "A mixed approach would be prompt selectors by langchain, which can invoke the best model for the job. Unfortunately, there's no docs for this, but this is a viable plan for prod scale deployments and reducing costs.\n",
    "\n",
    "My agent type is zero shot, since future data is unknown, and having scalable prompts and agent pipelines is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce3b816-8d4a-45c8-be30-e5bde1e8c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "import pandas as pd\n",
    "import os\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce771325",
   "metadata": {},
   "source": [
    "### Chat template improvement\n",
    "\n",
    "for a prod scale system, using CustomChatTemplate from Langchain is the best approach to take. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1933d65-2a97-42de-b2fa-12442fda4987",
   "metadata": {},
   "source": [
    "### For prod deployment fact checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c344e2ad-b5ac-4e29-9d88-e20b431118a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM (Language Model)\n",
    "llm = ChatOpenAI(model=\"gpt-4-0613\",temperature=0.3)\n",
    "# Initialize the LLMCheckerChain\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b15d53be-aeb3-49d5-9e70-262205d94db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]= ''\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"]=\"\"\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "prompt = \"\"\"\n",
    "### Instructions:\n",
    "\n",
    "As a User Researcher, your expertise lies in analyzing datasets for user research. Your task is to transform a user researcher's question into a Python code snippet using pandas. This code snippet should be designed to extract insights from the provided datasets using a blend of qualitative and quantitative methods.\n",
    "\n",
    "**Important Guidelines**:\n",
    "\n",
    "- **Contextual Understanding**: Recognize the context of each question and determine which dataset it pertains to. Choose the appropriate dataset for extraction.\n",
    "- **Clarity and Thoroughness**: Ensure clarity in the code. Make use of clear variable names, and break down the code into smaller chunks if necessary for better readability. Deliver thorough, actionable insights with every interaction.\n",
    "- **Annotations and Comments**: Each code snippet should have comments explaining the steps being taken, so you can understand and potentially modify it in the future.\n",
    "- **Visual Representations**: When visual clarity is beneficial or suggested by the question, produce graphs, plots, and histograms using libraries like `matplotlib` or `seaborn`.\n",
    "- **Optimization**: Ensure that the code is optimized for performance, especially when dealing with large datasets.\n",
    "\n",
    "### Input:\n",
    "Translate the user researcher's question: {input} to extract the desired information from the datasets.\n",
    "\n",
    "### Response:\n",
    "Your response should include graphs, plots, and histograms, as well as a summary of your results. Exclude code outputs, and give a comprehensive understanding of your results as a User Researcher.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74429b",
   "metadata": {},
   "source": [
    "### Toolkits\n",
    "\n",
    "Why have i gone for pandas agents instead of using llm-math?\n",
    "\n",
    "- LLM-Math is not optimised for large datasets and performing complex operations which pandas can handle easily.\n",
    "- If LLM-Math has to be implemented, I'd recommend having a list of quantatative operation functions created and invoked by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2b852-42de-4402-8593-6190464d4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM (Language Model)\n",
    "llm = ChatOpenAI(model=\"gpt-4-0613\",temperature=0.3)\n",
    "# Initialize the LLMCheckerChain\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a343211b-0d56-49de-83d7-a7c457c11e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q.yearborn</th>\n",
       "      <th>Q.gender</th>\n",
       "      <th>Q.ethnicity</th>\n",
       "      <th>What do you use to stream TV shows or movies? Select all that apply.</th>\n",
       "      <th>Of the two options, how do you identify?</th>\n",
       "      <th>Which statement applies to you? Select one.</th>\n",
       "      <th>Which do you currently use? Select all that apply.</th>\n",
       "      <th>Of the below list, which platform do you use the most, right now? Select one.</th>\n",
       "      <th>Which do you currently use? Select all that apply..1</th>\n",
       "      <th>Of the below list, which platform do you use the most, right now? Select one..1</th>\n",
       "      <th>...</th>\n",
       "      <th>How many hours per week do you spend watching TV shows?</th>\n",
       "      <th>How many hours per week do you spend watching movies?</th>\n",
       "      <th>How many hours per week do you spend playing video games?</th>\n",
       "      <th>How many hours per week do you spend listening to podcasts or audiobooks?</th>\n",
       "      <th>How many hours per week do you spend listening to music?</th>\n",
       "      <th>Of the below list, which is your favorite to do?</th>\n",
       "      <th>When you are streaming a TV show or movie, which do you primarily prefer?</th>\n",
       "      <th>When you are streaming a TV show or movie, which do you primarily prefer?.1</th>\n",
       "      <th>When you discover a new TV show or movie, how do you typically first hear about it? Select one.</th>\n",
       "      <th>How important is it to you to be the first of your friend group to discover a new TV show or movie?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>Female</td>\n",
       "      <td>['Black or African American']</td>\n",
       "      <td>['Smart TV', 'Roku ', 'Chromecast', 'Firestick...</td>\n",
       "      <td>Extrovert</td>\n",
       "      <td>I am a full time student</td>\n",
       "      <td>['Hulu', 'Netflix', 'Amazon Prime Video', 'You...</td>\n",
       "      <td>Paramount +</td>\n",
       "      <td>['Spotify', 'Apple Music', 'Pandora', 'SoundCl...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>...</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>12+ hours</td>\n",
       "      <td>0 hours</td>\n",
       "      <td>12+ hours</td>\n",
       "      <td>Listening to music</td>\n",
       "      <td>Focus entirely on the show</td>\n",
       "      <td>Watch alone</td>\n",
       "      <td>Hearing about it from a friend/family member</td>\n",
       "      <td>Not at all important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Female</td>\n",
       "      <td>['Black or African American']</td>\n",
       "      <td>['Roku ', 'Smartphone', 'Laptop']</td>\n",
       "      <td>Introvert</td>\n",
       "      <td>I have a part time job</td>\n",
       "      <td>['Hulu', 'Netflix', 'Amazon Prime Video', 'Sli...</td>\n",
       "      <td>Amazon Prime Video</td>\n",
       "      <td>['Apple Music']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>0 hours</td>\n",
       "      <td>0 hours</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>Listening to music</td>\n",
       "      <td>Focus entirely on the show</td>\n",
       "      <td>Watch alone</td>\n",
       "      <td>Seeing it on social media or online</td>\n",
       "      <td>Somewhat important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>Female</td>\n",
       "      <td>['Black or African American']</td>\n",
       "      <td>['Smart TV', 'Roku ', 'Chromecast', 'Apple TV'...</td>\n",
       "      <td>Extrovert</td>\n",
       "      <td>I have a full time job</td>\n",
       "      <td>['Hulu', 'Netflix', 'YouTube Premium', 'Peacoc...</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>['Amazon Music', 'Apple Music', 'Pandora', 'Yo...</td>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>...</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>0 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>12+ hours</td>\n",
       "      <td>Listening to music</td>\n",
       "      <td>Have it on in the background</td>\n",
       "      <td>Watch with friends/family</td>\n",
       "      <td>Seeing it on social media or online</td>\n",
       "      <td>Very important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>Female</td>\n",
       "      <td>['Black or African American']</td>\n",
       "      <td>['Apple TV', 'Gaming console', 'Smartphone', '...</td>\n",
       "      <td>Introvert</td>\n",
       "      <td>I am a full time student</td>\n",
       "      <td>['Apple TV+', 'Disney+', 'Funimation']</td>\n",
       "      <td>Apple TV+</td>\n",
       "      <td>['Spotify', 'YouTube Music']</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>...</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>1-5 hours</td>\n",
       "      <td>Watching TV shows</td>\n",
       "      <td>Focus entirely on the show</td>\n",
       "      <td>Watch with friends/family</td>\n",
       "      <td>Seeing it on social media or online</td>\n",
       "      <td>Very important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>Female</td>\n",
       "      <td>['White or Caucasian']</td>\n",
       "      <td>['Roku ', 'Firestick', 'Smartphone', 'Laptop']</td>\n",
       "      <td>Extrovert</td>\n",
       "      <td>I have a part time job</td>\n",
       "      <td>['Hulu', 'Netflix', 'Sling', 'Disney+', 'Param...</td>\n",
       "      <td>Paramount +</td>\n",
       "      <td>['Spotify', 'Apple Music', 'Pandora', 'YouTube...</td>\n",
       "      <td>Apple Music</td>\n",
       "      <td>...</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>6-8 hours</td>\n",
       "      <td>&lt; 1 hour</td>\n",
       "      <td>0 hours</td>\n",
       "      <td>9-11 hours</td>\n",
       "      <td>Watching TV shows</td>\n",
       "      <td>Have it on in the background</td>\n",
       "      <td>Watch alone</td>\n",
       "      <td>Seeing it on social media or online</td>\n",
       "      <td>Somewhat important</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q.yearborn Q.gender                    Q.ethnicity  \\\n",
       "0        2004   Female  ['Black or African American']   \n",
       "1        2000   Female  ['Black or African American']   \n",
       "2        1998   Female  ['Black or African American']   \n",
       "3        2002   Female  ['Black or African American']   \n",
       "4        2002   Female         ['White or Caucasian']   \n",
       "\n",
       "  What do you use to stream TV shows or movies? Select all that apply.  \\\n",
       "0  ['Smart TV', 'Roku ', 'Chromecast', 'Firestick...                     \n",
       "1                  ['Roku ', 'Smartphone', 'Laptop']                     \n",
       "2  ['Smart TV', 'Roku ', 'Chromecast', 'Apple TV'...                     \n",
       "3  ['Apple TV', 'Gaming console', 'Smartphone', '...                     \n",
       "4     ['Roku ', 'Firestick', 'Smartphone', 'Laptop']                     \n",
       "\n",
       "  Of the two options, how do you identify?  \\\n",
       "0                                Extrovert   \n",
       "1                                Introvert   \n",
       "2                                Extrovert   \n",
       "3                                Introvert   \n",
       "4                                Extrovert   \n",
       "\n",
       "  Which statement applies to you? Select one.  \\\n",
       "0                    I am a full time student   \n",
       "1                      I have a part time job   \n",
       "2                      I have a full time job   \n",
       "3                    I am a full time student   \n",
       "4                      I have a part time job   \n",
       "\n",
       "  Which do you currently use? Select all that apply.  \\\n",
       "0  ['Hulu', 'Netflix', 'Amazon Prime Video', 'You...   \n",
       "1  ['Hulu', 'Netflix', 'Amazon Prime Video', 'Sli...   \n",
       "2  ['Hulu', 'Netflix', 'YouTube Premium', 'Peacoc...   \n",
       "3             ['Apple TV+', 'Disney+', 'Funimation']   \n",
       "4  ['Hulu', 'Netflix', 'Sling', 'Disney+', 'Param...   \n",
       "\n",
       "  Of the below list, which platform do you use the most, right now? Select one.  \\\n",
       "0                                        Paramount +                              \n",
       "1                                 Amazon Prime Video                              \n",
       "2                                            Netflix                              \n",
       "3                                          Apple TV+                              \n",
       "4                                        Paramount +                              \n",
       "\n",
       "  Which do you currently use? Select all that apply..1  \\\n",
       "0  ['Spotify', 'Apple Music', 'Pandora', 'SoundCl...     \n",
       "1                                    ['Apple Music']     \n",
       "2  ['Amazon Music', 'Apple Music', 'Pandora', 'Yo...     \n",
       "3                       ['Spotify', 'YouTube Music']     \n",
       "4  ['Spotify', 'Apple Music', 'Pandora', 'YouTube...     \n",
       "\n",
       "  Of the below list, which platform do you use the most, right now? Select one..1  \\\n",
       "0                                            Spotify                                \n",
       "1                                                NaN                                \n",
       "2                                      YouTube Music                                \n",
       "3                                            Spotify                                \n",
       "4                                        Apple Music                                \n",
       "\n",
       "   ... How many hours per week do you spend watching TV shows?  \\\n",
       "0  ...                                         9-11 hours        \n",
       "1  ...                                          1-5 hours        \n",
       "2  ...                                          1-5 hours        \n",
       "3  ...                                          1-5 hours        \n",
       "4  ...                                         9-11 hours        \n",
       "\n",
       "  How many hours per week do you spend watching movies?  \\\n",
       "0                                         9-11 hours      \n",
       "1                                          1-5 hours      \n",
       "2                                         9-11 hours      \n",
       "3                                          1-5 hours      \n",
       "4                                          6-8 hours      \n",
       "\n",
       "  How many hours per week do you spend playing video games?  \\\n",
       "0                                          12+ hours          \n",
       "1                                            0 hours          \n",
       "2                                            0 hours          \n",
       "3                                          1-5 hours          \n",
       "4                                           < 1 hour          \n",
       "\n",
       "  How many hours per week do you spend listening to podcasts or audiobooks?  \\\n",
       "0                                            0 hours                          \n",
       "1                                            0 hours                          \n",
       "2                                          1-5 hours                          \n",
       "3                                          1-5 hours                          \n",
       "4                                            0 hours                          \n",
       "\n",
       "  How many hours per week do you spend listening to music?  \\\n",
       "0                                          12+ hours         \n",
       "1                                         9-11 hours         \n",
       "2                                          12+ hours         \n",
       "3                                          1-5 hours         \n",
       "4                                         9-11 hours         \n",
       "\n",
       "  Of the below list, which is your favorite to do?  \\\n",
       "0                               Listening to music   \n",
       "1                               Listening to music   \n",
       "2                               Listening to music   \n",
       "3                                Watching TV shows   \n",
       "4                                Watching TV shows   \n",
       "\n",
       "  When you are streaming a TV show or movie, which do you primarily prefer?  \\\n",
       "0                         Focus entirely on the show                          \n",
       "1                         Focus entirely on the show                          \n",
       "2                       Have it on in the background                          \n",
       "3                         Focus entirely on the show                          \n",
       "4                       Have it on in the background                          \n",
       "\n",
       "  When you are streaming a TV show or movie, which do you primarily prefer?.1  \\\n",
       "0                                        Watch alone                            \n",
       "1                                        Watch alone                            \n",
       "2                          Watch with friends/family                            \n",
       "3                          Watch with friends/family                            \n",
       "4                                        Watch alone                            \n",
       "\n",
       "  When you discover a new TV show or movie, how do you typically first hear about it? Select one.  \\\n",
       "0       Hearing about it from a friend/family member                                                \n",
       "1                Seeing it on social media or online                                                \n",
       "2                Seeing it on social media or online                                                \n",
       "3                Seeing it on social media or online                                                \n",
       "4                Seeing it on social media or online                                                \n",
       "\n",
       "  How important is it to you to be the first of your friend group to discover a new TV show or movie?  \n",
       "0                               Not at all important                                                   \n",
       "1                                 Somewhat important                                                   \n",
       "2                                     Very important                                                   \n",
       "3                                     Very important                                                   \n",
       "4                                 Somewhat important                                                   \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('/home/rachitt/qual-llm/raw_data/survey_1.csv')\n",
    "df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714cdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_1_instruct = create_pandas_dataframe_agent(OpenAI(model=\"text-davinci-003\", temperature=0.3), df_1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48121f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_1_instruct.run(f\"{prompt} what is the average age of the respondents? Show as a graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a394d4-9768-4e87-9899-4c46010afd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_agent = create_pandas_dataframe_agent(ChatOpenAI(model=\"gpt-4-0613\",temperature=0.3), df_1, verbose=True,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59b2679d-91f5-499f-bb34-e8ac64fc6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to count the number of respondents who prefer to watch alone and those who prefer to watch with friends/family. The relevant column in the dataframe is 'When you are streaming a TV show or movie, which do you primarily prefer?'.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df['When you are streaming a TV show or movie, which do you primarily prefer?'].value_counts()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFocus entirely on the show      147\n",
      "Have it on in the background    114\n",
      "Name: When you are streaming a TV show or movie, which do you primarily prefer?, dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe output does not match the expected categories of 'Watch alone' and 'Watch with friends/family'. It seems I have used the wrong column. The correct column should be 'When you are streaming a TV show or movie, which do you primarily prefer?.1'. I will repeat the previous action with the correct column.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df['When you are streaming a TV show or movie, which do you primarily prefer?.1'].value_counts()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWatch alone                  138\n",
      "Watch with friends/family    123\n",
      "Name: When you are streaming a TV show or movie, which do you primarily prefer?.1, dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I have the counts of respondents who prefer to watch alone and those who prefer to watch with friends/family. I will now plot these counts in a bar graph for better visualization.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Get the counts\n",
      "counts = df['When you are streaming a TV show or movie, which do you primarily prefer?.1'].value_counts()\n",
      "\n",
      "# Create the bar plot\n",
      "plt.bar(counts.index, counts.values)\n",
      "plt.xlabel('Preference')\n",
      "plt.ylabel('Number of Respondents')\n",
      "plt.title('Number of Respondents by Watching Preference')\n",
      "plt.show()\n",
      "```\u001b[0m"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have successfully generated a bar graph showing the number of respondents who prefer to watch alone vs. with friends/family. The graph shows that the number of respondents who prefer to watch alone is slightly higher than those who prefer to watch with friends/family.\n",
      "\n",
      "Final Answer: There are 138 respondents who prefer to watch alone and 123 respondents who prefer to watch with friends/family. The bar graph visually represents these counts.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_1_string = pd_agent.run(f\"{prompt} How many respondents prefer to watch alone vs. with friends/family, show a graph too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530962b2-7a58-4c4b-b916-79bd25e7c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1_string)\n",
    "checker_chain.run(df_1_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d25c88-b97f-4811-8f2b-faca569e9ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>In what year were you born?</th>\n",
       "      <th>Gender:</th>\n",
       "      <th>Race/Ethnicity:(Select all that apply)</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..6</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..6</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..6</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..6</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..5</th>\n",
       "      <th>Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..6</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite.</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..1</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..2</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..3</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..4</th>\n",
       "      <th>Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>Male</td>\n",
       "      <td>['White or Caucasian']</td>\n",
       "      <td>Chipotle</td>\n",
       "      <td>Del Taco</td>\n",
       "      <td>El Pollo Loco</td>\n",
       "      <td>Taco Johns</td>\n",
       "      <td>Qdoba</td>\n",
       "      <td>Moeâ€™s Southwest Grill</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>In-N-Out</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>Whataburger</td>\n",
       "      <td>Sonic</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Churchs</td>\n",
       "      <td>Raising Canes</td>\n",
       "      <td>McDonalds</td>\n",
       "      <td>KFC</td>\n",
       "      <td>Chick-fil-a</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Wendys</td>\n",
       "      <td>Ciciâ€™s Pizza</td>\n",
       "      <td>Domino's</td>\n",
       "      <td>Little Caesars</td>\n",
       "      <td>Pizza Hut</td>\n",
       "      <td>Blaze Pizza</td>\n",
       "      <td>Papa John's</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Sunoco</td>\n",
       "      <td>QuikTrip</td>\n",
       "      <td>Circle K</td>\n",
       "      <td>AmPm</td>\n",
       "      <td>Love's Travel Stops</td>\n",
       "      <td>Flying-J</td>\n",
       "      <td>Jersey Mike's Subs</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>Jimmy John's</td>\n",
       "      <td>Panera</td>\n",
       "      <td>Potbelly</td>\n",
       "      <td>Subway</td>\n",
       "      <td>Which Wich</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>IHOP</td>\n",
       "      <td>Waffle House</td>\n",
       "      <td>Cracker Barrel</td>\n",
       "      <td>Village Inn</td>\n",
       "      <td>Perkins Restaurant &amp; Bakery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>Male</td>\n",
       "      <td>['White or Caucasian', 'Hispanic']</td>\n",
       "      <td>Chipotle</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>Moeâ€™s Southwest Grill</td>\n",
       "      <td>Qdoba</td>\n",
       "      <td>El Pollo Loco</td>\n",
       "      <td>Taco Johns</td>\n",
       "      <td>Del Taco</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>In-N-Out</td>\n",
       "      <td>Sonic</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>Whataburger</td>\n",
       "      <td>Shake Shack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KFC</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>McDonalds</td>\n",
       "      <td>Churchs</td>\n",
       "      <td>Chick-fil-a</td>\n",
       "      <td>Raising Canes</td>\n",
       "      <td>Wendys</td>\n",
       "      <td>Little Caesars</td>\n",
       "      <td>Papa John's</td>\n",
       "      <td>Domino's</td>\n",
       "      <td>Pizza Hut</td>\n",
       "      <td>Blaze Pizza</td>\n",
       "      <td>Ciciâ€™s Pizza</td>\n",
       "      <td>Flying-J</td>\n",
       "      <td>Sunoco</td>\n",
       "      <td>QuikTrip</td>\n",
       "      <td>Love's Travel Stops</td>\n",
       "      <td>Circle K</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>AmPm</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>Jimmy John's</td>\n",
       "      <td>Subway</td>\n",
       "      <td>Potbelly</td>\n",
       "      <td>Panera</td>\n",
       "      <td>Jersey Mike's Subs</td>\n",
       "      <td>Which Wich</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>IHOP</td>\n",
       "      <td>Perkins Restaurant &amp; Bakery</td>\n",
       "      <td>Cracker Barrel</td>\n",
       "      <td>Waffle House</td>\n",
       "      <td>Village Inn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>Male</td>\n",
       "      <td>['White or Caucasian']</td>\n",
       "      <td>El Pollo Loco</td>\n",
       "      <td>Del Taco</td>\n",
       "      <td>Taco Johns</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>Qdoba</td>\n",
       "      <td>Moeâ€™s Southwest Grill</td>\n",
       "      <td>Chipotle</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>Sonic</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>Whataburger</td>\n",
       "      <td>In-N-Out</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wendys</td>\n",
       "      <td>KFC</td>\n",
       "      <td>McDonalds</td>\n",
       "      <td>Chick-fil-a</td>\n",
       "      <td>Raising Canes</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Churchs</td>\n",
       "      <td>Ciciâ€™s Pizza</td>\n",
       "      <td>Papa John's</td>\n",
       "      <td>Domino's</td>\n",
       "      <td>Pizza Hut</td>\n",
       "      <td>Blaze Pizza</td>\n",
       "      <td>Little Caesars</td>\n",
       "      <td>Flying-J</td>\n",
       "      <td>QuikTrip</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Love's Travel Stops</td>\n",
       "      <td>Circle K</td>\n",
       "      <td>Sunoco</td>\n",
       "      <td>AmPm</td>\n",
       "      <td>Jimmy John's</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>Potbelly</td>\n",
       "      <td>Which Wich</td>\n",
       "      <td>Jersey Mike's Subs</td>\n",
       "      <td>Subway</td>\n",
       "      <td>Panera</td>\n",
       "      <td>Waffle House</td>\n",
       "      <td>Cracker Barrel</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>Perkins Restaurant &amp; Bakery</td>\n",
       "      <td>IHOP</td>\n",
       "      <td>Village Inn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>Male</td>\n",
       "      <td>['White or Caucasian', 'Other (please specify):']</td>\n",
       "      <td>Qdoba</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>Taco Johns</td>\n",
       "      <td>Del Taco</td>\n",
       "      <td>Moeâ€™s Southwest Grill</td>\n",
       "      <td>El Pollo Loco</td>\n",
       "      <td>Chipotle</td>\n",
       "      <td>In-N-Out</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Sonic</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>Whataburger</td>\n",
       "      <td>Shake Shack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raising Canes</td>\n",
       "      <td>Churchs</td>\n",
       "      <td>McDonalds</td>\n",
       "      <td>Wendys</td>\n",
       "      <td>KFC</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Chick-fil-a</td>\n",
       "      <td>Pizza Hut</td>\n",
       "      <td>Ciciâ€™s Pizza</td>\n",
       "      <td>Domino's</td>\n",
       "      <td>Little Caesars</td>\n",
       "      <td>Blaze Pizza</td>\n",
       "      <td>Papa John's</td>\n",
       "      <td>Circle K</td>\n",
       "      <td>Love's Travel Stops</td>\n",
       "      <td>Sunoco</td>\n",
       "      <td>QuikTrip</td>\n",
       "      <td>Flying-J</td>\n",
       "      <td>AmPm</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Jimmy John's</td>\n",
       "      <td>Which Wich</td>\n",
       "      <td>Jersey Mike's Subs</td>\n",
       "      <td>Panera</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>Subway</td>\n",
       "      <td>Potbelly</td>\n",
       "      <td>IHOP</td>\n",
       "      <td>Perkins Restaurant &amp; Bakery</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>Village Inn</td>\n",
       "      <td>Waffle House</td>\n",
       "      <td>Cracker Barrel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>Male</td>\n",
       "      <td>['White or Caucasian']</td>\n",
       "      <td>Del Taco</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>Moeâ€™s Southwest Grill</td>\n",
       "      <td>El Pollo Loco</td>\n",
       "      <td>Chipotle</td>\n",
       "      <td>Taco Johns</td>\n",
       "      <td>Qdoba</td>\n",
       "      <td>Whataburger</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>Shake Shack</td>\n",
       "      <td>In-N-Out</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>Sonic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KFC</td>\n",
       "      <td>Raising Canes</td>\n",
       "      <td>McDonalds</td>\n",
       "      <td>Chick-fil-a</td>\n",
       "      <td>Wendys</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Churchs</td>\n",
       "      <td>Domino's</td>\n",
       "      <td>Little Caesars</td>\n",
       "      <td>Papa John's</td>\n",
       "      <td>Pizza Hut</td>\n",
       "      <td>Ciciâ€™s Pizza</td>\n",
       "      <td>Blaze Pizza</td>\n",
       "      <td>Love's Travel Stops</td>\n",
       "      <td>AmPm</td>\n",
       "      <td>Sunoco</td>\n",
       "      <td>QuikTrip</td>\n",
       "      <td>Circle K</td>\n",
       "      <td>Flying-J</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Jersey Mike's Subs</td>\n",
       "      <td>Jimmy John's</td>\n",
       "      <td>Subway</td>\n",
       "      <td>Panera</td>\n",
       "      <td>Potbelly</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>Which Wich</td>\n",
       "      <td>IHOP</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>Waffle House</td>\n",
       "      <td>Village Inn</td>\n",
       "      <td>Perkins Restaurant &amp; Bakery</td>\n",
       "      <td>Cracker Barrel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   In what year were you born? Gender:  \\\n",
       "0                         2003    Male   \n",
       "1                         1998    Male   \n",
       "2                         2003    Male   \n",
       "3                         2001    Male   \n",
       "4                         1999    Male   \n",
       "\n",
       "              Race/Ethnicity:(Select all that apply)  \\\n",
       "0                             ['White or Caucasian']   \n",
       "1                 ['White or Caucasian', 'Hispanic']   \n",
       "2                             ['White or Caucasian']   \n",
       "3  ['White or Caucasian', 'Other (please specify):']   \n",
       "4                             ['White or Caucasian']   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite.  \\\n",
       "0                                           Chipotle                                                                                                                                            \n",
       "1                                           Chipotle                                                                                                                                            \n",
       "2                                      El Pollo Loco                                                                                                                                            \n",
       "3                                              Qdoba                                                                                                                                            \n",
       "4                                           Del Taco                                                                                                                                            \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..1  \\\n",
       "0                                           Del Taco                                                                                                                                              \n",
       "1                                          Taco Bell                                                                                                                                              \n",
       "2                                           Del Taco                                                                                                                                              \n",
       "3                                          Taco Bell                                                                                                                                              \n",
       "4                                          Taco Bell                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..2  \\\n",
       "0                                      El Pollo Loco                                                                                                                                              \n",
       "1                              Moeâ€™s Southwest Grill                                                                                                                                              \n",
       "2                                         Taco Johns                                                                                                                                              \n",
       "3                                         Taco Johns                                                                                                                                              \n",
       "4                              Moeâ€™s Southwest Grill                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..3  \\\n",
       "0                                         Taco Johns                                                                                                                                              \n",
       "1                                              Qdoba                                                                                                                                              \n",
       "2                                          Taco Bell                                                                                                                                              \n",
       "3                                           Del Taco                                                                                                                                              \n",
       "4                                      El Pollo Loco                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..4  \\\n",
       "0                                              Qdoba                                                                                                                                              \n",
       "1                                      El Pollo Loco                                                                                                                                              \n",
       "2                                              Qdoba                                                                                                                                              \n",
       "3                              Moeâ€™s Southwest Grill                                                                                                                                              \n",
       "4                                           Chipotle                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..5  \\\n",
       "0                              Moeâ€™s Southwest Grill                                                                                                                                              \n",
       "1                                         Taco Johns                                                                                                                                              \n",
       "2                              Moeâ€™s Southwest Grill                                                                                                                                              \n",
       "3                                      El Pollo Loco                                                                                                                                              \n",
       "4                                         Taco Johns                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, please drag and drop the following Mexican fast food restaurants from favorite to least favorite..6  \\\n",
       "0                                          Taco Bell                                                                                                                                              \n",
       "1                                           Del Taco                                                                                                                                              \n",
       "2                                           Chipotle                                                                                                                                              \n",
       "3                                           Chipotle                                                                                                                                              \n",
       "4                                              Qdoba                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite.  \\\n",
       "0                                           In-N-Out                                                                                                                                           \n",
       "1                                          Five Guys                                                                                                                                           \n",
       "2                                          Five Guys                                                                                                                                           \n",
       "3                                           In-N-Out                                                                                                                                           \n",
       "4                                        Whataburger                                                                                                                                           \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..1  \\\n",
       "0                                        Burger King                                                                                                                                             \n",
       "1                                           In-N-Out                                                                                                                                             \n",
       "2                                              Sonic                                                                                                                                             \n",
       "3                                         McDonald's                                                                                                                                             \n",
       "4                                          Five Guys                                                                                                                                             \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..2  \\\n",
       "0                                        Whataburger                                                                                                                                             \n",
       "1                                              Sonic                                                                                                                                             \n",
       "2                                         McDonald's                                                                                                                                             \n",
       "3                                              Sonic                                                                                                                                             \n",
       "4                                        Shake Shack                                                                                                                                             \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..3  \\\n",
       "0                                              Sonic                                                                                                                                             \n",
       "1                                        Burger King                                                                                                                                             \n",
       "2                                        Burger King                                                                                                                                             \n",
       "3                                        Burger King                                                                                                                                             \n",
       "4                                           In-N-Out                                                                                                                                             \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..4  \\\n",
       "0                                         McDonald's                                                                                                                                             \n",
       "1                                        Whataburger                                                                                                                                             \n",
       "2                                        Whataburger                                                                                                                                             \n",
       "3                                        Whataburger                                                                                                                                             \n",
       "4                                        Burger King                                                                                                                                             \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..5  \\\n",
       "0                                          Five Guys                                                                                                                                             \n",
       "1                                        Shake Shack                                                                                                                                             \n",
       "2                                           In-N-Out                                                                                                                                             \n",
       "3                                        Shake Shack                                                                                                                                             \n",
       "4                                              Sonic                                                                                                                                             \n",
       "\n",
       "   Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following burger fast food restaurants from favorite to least favorite..6  \\\n",
       "0                                                NaN                                                                                                                                              \n",
       "1                                                NaN                                                                                                                                              \n",
       "2                                                NaN                                                                                                                                              \n",
       "3                                                NaN                                                                                                                                              \n",
       "4                                                NaN                                                                                                                                              \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite.  \\\n",
       "0                                            Churchs                                                                                                                                                                \n",
       "1                                                KFC                                                                                                                                                                \n",
       "2                                             Wendys                                                                                                                                                                \n",
       "3                                      Raising Canes                                                                                                                                                                \n",
       "4                                                KFC                                                                                                                                                                \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..1  \\\n",
       "0                                      Raising Canes                                                                                                                                                                  \n",
       "1                                            Popeyes                                                                                                                                                                  \n",
       "2                                                KFC                                                                                                                                                                  \n",
       "3                                            Churchs                                                                                                                                                                  \n",
       "4                                      Raising Canes                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..2  \\\n",
       "0                                          McDonalds                                                                                                                                                                  \n",
       "1                                          McDonalds                                                                                                                                                                  \n",
       "2                                          McDonalds                                                                                                                                                                  \n",
       "3                                          McDonalds                                                                                                                                                                  \n",
       "4                                          McDonalds                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..3  \\\n",
       "0                                                KFC                                                                                                                                                                  \n",
       "1                                            Churchs                                                                                                                                                                  \n",
       "2                                        Chick-fil-a                                                                                                                                                                  \n",
       "3                                             Wendys                                                                                                                                                                  \n",
       "4                                        Chick-fil-a                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..4  \\\n",
       "0                                        Chick-fil-a                                                                                                                                                                  \n",
       "1                                        Chick-fil-a                                                                                                                                                                  \n",
       "2                                      Raising Canes                                                                                                                                                                  \n",
       "3                                                KFC                                                                                                                                                                  \n",
       "4                                             Wendys                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..5  \\\n",
       "0                                            Popeyes                                                                                                                                                                  \n",
       "1                                      Raising Canes                                                                                                                                                                  \n",
       "2                                            Popeyes                                                                                                                                                                  \n",
       "3                                            Popeyes                                                                                                                                                                  \n",
       "4                                            Popeyes                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop your favorite chicken sandwich from these fast food restaurants from favorite to least favorite..6  \\\n",
       "0                                             Wendys                                                                                                                                                                  \n",
       "1                                             Wendys                                                                                                                                                                  \n",
       "2                                            Churchs                                                                                                                                                                  \n",
       "3                                        Chick-fil-a                                                                                                                                                                  \n",
       "4                                            Churchs                                                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite.  \\\n",
       "0                                       Ciciâ€™s Pizza                                                                                                                                \n",
       "1                                     Little Caesars                                                                                                                                \n",
       "2                                       Ciciâ€™s Pizza                                                                                                                                \n",
       "3                                          Pizza Hut                                                                                                                                \n",
       "4                                           Domino's                                                                                                                                \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..1  \\\n",
       "0                                           Domino's                                                                                                                                  \n",
       "1                                        Papa John's                                                                                                                                  \n",
       "2                                        Papa John's                                                                                                                                  \n",
       "3                                       Ciciâ€™s Pizza                                                                                                                                  \n",
       "4                                     Little Caesars                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..2  \\\n",
       "0                                     Little Caesars                                                                                                                                  \n",
       "1                                           Domino's                                                                                                                                  \n",
       "2                                           Domino's                                                                                                                                  \n",
       "3                                           Domino's                                                                                                                                  \n",
       "4                                        Papa John's                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..3  \\\n",
       "0                                          Pizza Hut                                                                                                                                  \n",
       "1                                          Pizza Hut                                                                                                                                  \n",
       "2                                          Pizza Hut                                                                                                                                  \n",
       "3                                     Little Caesars                                                                                                                                  \n",
       "4                                          Pizza Hut                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..4  \\\n",
       "0                                        Blaze Pizza                                                                                                                                  \n",
       "1                                        Blaze Pizza                                                                                                                                  \n",
       "2                                        Blaze Pizza                                                                                                                                  \n",
       "3                                        Blaze Pizza                                                                                                                                  \n",
       "4                                       Ciciâ€™s Pizza                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following pizza restaurants from favorite to least favorite..5  \\\n",
       "0                                        Papa John's                                                                                                                                  \n",
       "1                                       Ciciâ€™s Pizza                                                                                                                                  \n",
       "2                                     Little Caesars                                                                                                                                  \n",
       "3                                        Papa John's                                                                                                                                  \n",
       "4                                        Blaze Pizza                                                                                                                                  \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite.  \\\n",
       "0                                           7-Eleven                                                                                                                                 \n",
       "1                                           Flying-J                                                                                                                                 \n",
       "2                                           Flying-J                                                                                                                                 \n",
       "3                                           Circle K                                                                                                                                 \n",
       "4                                Love's Travel Stops                                                                                                                                 \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..1  \\\n",
       "0                                             Sunoco                                                                                                                                   \n",
       "1                                             Sunoco                                                                                                                                   \n",
       "2                                           QuikTrip                                                                                                                                   \n",
       "3                                Love's Travel Stops                                                                                                                                   \n",
       "4                                               AmPm                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..2  \\\n",
       "0                                           QuikTrip                                                                                                                                   \n",
       "1                                           QuikTrip                                                                                                                                   \n",
       "2                                           7-Eleven                                                                                                                                   \n",
       "3                                             Sunoco                                                                                                                                   \n",
       "4                                             Sunoco                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..3  \\\n",
       "0                                           Circle K                                                                                                                                   \n",
       "1                                Love's Travel Stops                                                                                                                                   \n",
       "2                                Love's Travel Stops                                                                                                                                   \n",
       "3                                           QuikTrip                                                                                                                                   \n",
       "4                                           QuikTrip                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..4  \\\n",
       "0                                               AmPm                                                                                                                                   \n",
       "1                                           Circle K                                                                                                                                   \n",
       "2                                           Circle K                                                                                                                                   \n",
       "3                                           Flying-J                                                                                                                                   \n",
       "4                                           Circle K                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..5  \\\n",
       "0                                Love's Travel Stops                                                                                                                                   \n",
       "1                                           7-Eleven                                                                                                                                   \n",
       "2                                             Sunoco                                                                                                                                   \n",
       "3                                               AmPm                                                                                                                                   \n",
       "4                                           Flying-J                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following convenience stores from favorite to least favorite..6  \\\n",
       "0                                           Flying-J                                                                                                                                   \n",
       "1                                               AmPm                                                                                                                                   \n",
       "2                                               AmPm                                                                                                                                   \n",
       "3                                           7-Eleven                                                                                                                                   \n",
       "4                                           7-Eleven                                                                                                                                   \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite.  \\\n",
       "0                                 Jersey Mike's Subs                                                                                                                             \n",
       "1                                     Firehouse Subs                                                                                                                             \n",
       "2                                       Jimmy John's                                                                                                                             \n",
       "3                                       Jimmy John's                                                                                                                             \n",
       "4                                 Jersey Mike's Subs                                                                                                                             \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..1  \\\n",
       "0                                     Firehouse Subs                                                                                                                               \n",
       "1                                       Jimmy John's                                                                                                                               \n",
       "2                                     Firehouse Subs                                                                                                                               \n",
       "3                                         Which Wich                                                                                                                               \n",
       "4                                       Jimmy John's                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..2  \\\n",
       "0                                       Jimmy John's                                                                                                                               \n",
       "1                                             Subway                                                                                                                               \n",
       "2                                           Potbelly                                                                                                                               \n",
       "3                                 Jersey Mike's Subs                                                                                                                               \n",
       "4                                             Subway                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..3  \\\n",
       "0                                             Panera                                                                                                                               \n",
       "1                                           Potbelly                                                                                                                               \n",
       "2                                         Which Wich                                                                                                                               \n",
       "3                                             Panera                                                                                                                               \n",
       "4                                             Panera                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..4  \\\n",
       "0                                           Potbelly                                                                                                                               \n",
       "1                                             Panera                                                                                                                               \n",
       "2                                 Jersey Mike's Subs                                                                                                                               \n",
       "3                                     Firehouse Subs                                                                                                                               \n",
       "4                                           Potbelly                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..5  \\\n",
       "0                                             Subway                                                                                                                               \n",
       "1                                 Jersey Mike's Subs                                                                                                                               \n",
       "2                                             Subway                                                                                                                               \n",
       "3                                             Subway                                                                                                                               \n",
       "4                                     Firehouse Subs                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 7 where 1 is your first choice and 7 is your least favorite choice, Please drag and drop the following sandwich shops from favorite to least favorite..6  \\\n",
       "0                                         Which Wich                                                                                                                               \n",
       "1                                         Which Wich                                                                                                                               \n",
       "2                                             Panera                                                                                                                               \n",
       "3                                           Potbelly                                                                                                                               \n",
       "4                                         Which Wich                                                                                                                               \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite.  \\\n",
       "0                                            Denny's                                                                                                                                    \n",
       "1                                            Denny's                                                                                                                                    \n",
       "2                                       Waffle House                                                                                                                                    \n",
       "3                                               IHOP                                                                                                                                    \n",
       "4                                               IHOP                                                                                                                                    \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..1  \\\n",
       "0                                               IHOP                                                                                                                                      \n",
       "1                                               IHOP                                                                                                                                      \n",
       "2                                     Cracker Barrel                                                                                                                                      \n",
       "3                        Perkins Restaurant & Bakery                                                                                                                                      \n",
       "4                                            Denny's                                                                                                                                      \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..2  \\\n",
       "0                                       Waffle House                                                                                                                                      \n",
       "1                        Perkins Restaurant & Bakery                                                                                                                                      \n",
       "2                                            Denny's                                                                                                                                      \n",
       "3                                            Denny's                                                                                                                                      \n",
       "4                                       Waffle House                                                                                                                                      \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..3  \\\n",
       "0                                     Cracker Barrel                                                                                                                                      \n",
       "1                                     Cracker Barrel                                                                                                                                      \n",
       "2                        Perkins Restaurant & Bakery                                                                                                                                      \n",
       "3                                        Village Inn                                                                                                                                      \n",
       "4                                        Village Inn                                                                                                                                      \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..4  \\\n",
       "0                                        Village Inn                                                                                                                                      \n",
       "1                                       Waffle House                                                                                                                                      \n",
       "2                                               IHOP                                                                                                                                      \n",
       "3                                       Waffle House                                                                                                                                      \n",
       "4                        Perkins Restaurant & Bakery                                                                                                                                      \n",
       "\n",
       "  Using a scale of 1 to 6 where 1 is your first choice and 6 is your least favorite choice, Please drag and drop the following breakfast restaurants from favorite to least favorite..5  \n",
       "0                        Perkins Restaurant & Bakery                                                                                                                                     \n",
       "1                                        Village Inn                                                                                                                                     \n",
       "2                                        Village Inn                                                                                                                                     \n",
       "3                                     Cracker Barrel                                                                                                                                     \n",
       "4                                     Cracker Barrel                                                                                                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('/home/rachitt/qual-llm/raw_data/survey_2.csv')\n",
    "df_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f65c4c0-92c0-4331-bdf2-9d4e6fda8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_instruct = create_pandas_dataframe_agent(OpenAI(model=\"text-davinci-003\", temperature=0.3), df_2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d15067-cac6-4c05-b728-df47cd559cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_instruct.run(f\"{prompt}Is Subway the most popular sandwich shop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d78a341-efa2-418c-9fc5-e6969a0d4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_agent = create_pandas_dataframe_agent(ChatOpenAI(model=\"gpt-4-0613\",temperature=0.5), df_2, verbose=True, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_2_agent.run(f\"{prompt} What is the gender split among folks who ranked Taco bell as their top choice? Plot a piechart to show this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efdcff",
   "metadata": {},
   "source": [
    "### Mutliple CSV agents\n",
    "\n",
    "A brute force way to index CSVs, which can be used for faster A/B testing. Not a recommended way, since we're using zero shot learning, due to which the model will lose context and produce lower quality results.\n",
    "\n",
    "There's also a known issue with multiple file agents is that for CSV agents, the OutputParser class from Langchain throws issues alot of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4331e79e-0431-4df3-8864-ef96146b138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "Multi_DF_agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    [df_1,df_2],\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb46785-d925-4bbe-8095-0f46dd4d96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_DF_agent.run(f\"{prompt} What is the gender split among folks who ranked Taco bell as their top choice? Plot a piechart to show this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc2efc-68d7-4390-b015-d8298f8b3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent, create_csv_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent = create_csv_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    [\"/home/rachitt/qual-llm/raw_data/survey_2.csv\", \"/home/rachitt/qual-llm/raw_data/survey_1.csv\"],\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7368be-86d9-445a-9018-d4da73a04c74",
   "metadata": {},
   "source": [
    "## Running Agents with monitoring\n",
    "\n",
    "Running all of our function calls with [https://www.trulens.org/], which allows evals+feedback for our LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6eb876-24fa-456d-80c2-9a2c9a9a93b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In language_match, input text1 will be set to *.__record__.main_input or `Select.RecordInput` .\n",
      "âœ… In language_match, input text2 will be set to *.__record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Initialize Huggingface-based feedback function collection class:\n",
    "hugs = Huggingface()\n",
    "\n",
    "# Define a language match feedback function using HuggingFace.\n",
    "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
    "# By default this will check language match on the main app input and main app\n",
    "# output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e58fbbf-abaa-42d4-80c5-465791575599",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_1_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m truchain_checker_1 \u001b[38;5;241m=\u001b[39m TruChain(checker_chain, \n\u001b[1;32m      2\u001b[0m     app_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKnit_AI_Workshop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     feedbacks\u001b[38;5;241m=\u001b[39m[f_lang_match],                                  \n\u001b[1;32m      4\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprototype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # Instrumented chain can operate like the original:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m truchain_checker_1(\u001b[43mdf_1_string\u001b[49m)\n\u001b[1;32m     11\u001b[0m display(llm_response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_1_string' is not defined"
     ]
    }
   ],
   "source": [
    "truchain_checker_1 = TruChain(checker_chain, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],                                  \n",
    "    tags = \"prototype\")\n",
    "\n",
    "# # Instrumented chain can operate like the original:\n",
    "# prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\n",
    "\n",
    "llm_response = truchain_checker_1(df_1_string)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275bd67-f1a1-497b-80cf-e19af237dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_instruct_survey_1 = TruChain(pd_1_instruct, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],                                  \n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\n",
    "\n",
    "llm_response = truchain_instruct_survey_1(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dc180-4a7b-4412-adb6-c634678ca7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_chat_survey_1 = TruChain(pd_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What is the gender split between people who use pay for the streaming service themselves vs share an account with someone?\"\n",
    "\n",
    "llm_response = truchain_chat_survey_1(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a164514-20fd-487c-ada3-04656eb80efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_instruct_survey_2 = TruChain(pd_2_instruct, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What is the gender split in the survey?\"\n",
    "\n",
    "llm_response = truchain_instruct_survey_2(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ce03b-e863-4271-aad8-fcc7cd14a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_chat_survey_2 = TruChain(pd_2_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} Is Subway the most popular sandwich shop?\"\n",
    "\n",
    "llm_response = truchain_chat_survey_2(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad06416-1af3-4bcb-88bd-30cb11a83e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "truchain_multiple_df = TruChain(Multi_DF_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the top three highest ranked mexican fast food restaurants? Plot a chart.?\"\n",
    "\n",
    "llm_response = truchain_multiple_df(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6fd75-b66e-4148-b9d5-96ef769c8d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2db6602e2e74f418f74994a443e1cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.29.59:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975ea65-28d4-439b-a2e8-28c863ef30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53e175-8392-473e-9485-46790903b760",
   "metadata": {},
   "source": [
    "### Vectorstore+Agents approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f0c0e-05cd-44d3-96ad-f65b616ba128",
   "metadata": {},
   "source": [
    "Vectorstores can significantly increase the context of the models, with a larger size.\n",
    "\n",
    "I wished to benchmark each approach to understand which approach performed the best on question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba3d408e-0f8d-486b-8810-cf01201bd9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                 | 2/4 [00:00<00:00, 227.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "loader = DirectoryLoader(\"/home/rachitt/qual-llm/raw_data\", glob=\"**/*.csv\",use_multithreading=True, loader_cls=CSVLoader, show_progress=True)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"survey_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c21e-572d-431d-887b-e75449524d12",
   "metadata": {},
   "source": [
    "agents should have been of type OpenAI functions for better compatibility with toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d51fa5d-f1cb-45dd-9cba-67ad9f31daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def checker_tool_function(text: str) -> str:\n",
    "    from langchain.chains import LLMCheckerChain\n",
    "    from langchain.llms import OpenAI\n",
    "\n",
    "    # Initialize the LLM (Language Model)\n",
    "    llm = OpenAI(temperature=0.3)\n",
    "\n",
    "    # Initialize the LLMCheckerChain\n",
    "    checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "\n",
    "    # Run the chain with the given text\n",
    "    return checker_chain.run(text)\n",
    "\n",
    "pandas_tool = [\n",
    "    Tool(\n",
    "        name='Pandas Data frame tool',\n",
    "        func=pd_agent.run,\n",
    "        description=\"Useful for when you need to answer questions about survey_1 about data on media consumption\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Pandas Data frame tool\",\n",
    "        func=pd_2_agent.run,\n",
    "        description=\"useful for when you need to answer questions about survey_2 about data on eating habits.\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]\n",
    "\n",
    "checker_tool = Tool.from_function(\n",
    "    func=checker_tool_function,\n",
    "    name=\"LLMChecker\",\n",
    "    description=\"Checks and validates the response from LLM\"\n",
    ")\n",
    "\n",
    "# Initialize the tools list and then append the tools to it\n",
    "tools = []\n",
    "tools.extend(pandas_tool)\n",
    "tools.append(checker_tool)\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31543182-be3d-40ec-96ab-696e2bebdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Env. var. OPENAI_API_KEY set from openai.api_key .\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'langchain.agents.agent.AgentExecutor'> at 0x7f8aeb142e40 is calling an instrumented method <function Chain.__call__ at 0x7f8afbe3c040>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x7f8ae84a7d00) using this function.\n",
      "A new object of type <class 'langchain.agents.agent.AgentExecutor'> at 0x7f8aeb142e40 is calling an instrumented method <function AgentExecutor._call at 0x7f8af79b5090>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x7f8ae84a7d00) using this function.\n",
      "A new object of type <class 'langchain.chains.llm.LLMChain'> at 0x7f8aeb143b80 is calling an instrumented method <function Chain.__call__ at 0x7f8afbe3c040>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app based on other object (0x7f8ae84a7d00) using this function.\n",
      "A new object of type <class 'langchain.chains.llm.LLMChain'> at 0x7f8aeb143b80 is calling an instrumented method <function LLMChain._call at 0x7f8afbe3c820>. The path of this call may be incorrect.\n",
      "Guessing path of new object is *.app.agent.llm_chain based on other object (0x7f8ae84a6480) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to find out what the most common reasons for cancelling a streaming subscription are.\n",
      "Action: Pandas Data frame tool\n",
      "Action Input: survey_1\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error calling wrapped function _call.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "Error calling wrapped function __call__.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "Error calling wrapped function _call.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 833, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 456, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 256, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "Error calling wrapped function __call__.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 833, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 456, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 256, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "Error calling wrapped function _call.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 891, in _take_next_step\n",
      "    observation = tool.run(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 369, in run\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 341, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 511, in _run\n",
      "    self.func(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 475, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 833, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 456, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 256, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "Error calling wrapped function __call__.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 891, in _take_next_step\n",
      "    observation = tool.run(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 369, in run\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 341, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 511, in _run\n",
      "    self.func(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 475, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 833, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 456, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 256, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n",
      "App raised an exception: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py\", line 682, in with_record\n",
      "    ret, cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 331, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 274, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 426, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py\", line 683, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 891, in _take_next_step\n",
      "    observation = tool.run(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 369, in run\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 341, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py\", line 511, in _run\n",
      "    self.func(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 475, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 1036, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 833, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py\", line 456, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 256, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 282, in __call__\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py\", line 276, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 846, in wrapper\n",
      "    raise error\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py\", line 796, in wrapper\n",
      "    rets = func(*bindings.args, **bindings.kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 415, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 310, in generate\n",
      "    raise e\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 300, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py\", line 447, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 340, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 279, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py\", line 277, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 702, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rachitt/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Instrumented chain can operate like the original:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m prompt_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m \u001b[43mtruchain_qa_vectordb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m display(llm_response)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/tru_chain.py:200\u001b[0m, in \u001b[0;36mTruChain.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Wrapped call to self.app._call with instrumentation. If you need to\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    get the record, use `call_with_record` instead. \u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/tru_chain.py:205\u001b[0m, in \u001b[0;36mTruChain._call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 205\u001b[0m     ret, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_record\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/tru_chain.py:190\u001b[0m, in \u001b[0;36mTruChain.call_with_record\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_with_record\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Record]:\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    Run the chain call method and also return a record metadata object.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_record\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py:714\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    711\u001b[0m     ret_record_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_error\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jsonify(error)\n\u001b[1;32m    713\u001b[0m perf \u001b[38;5;241m=\u001b[39m Perf(start_time\u001b[38;5;241m=\u001b[39mstart_time, end_time\u001b[38;5;241m=\u001b[39mend_time)\n\u001b[0;32m--> 714\u001b[0m ret_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_record\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mret_record_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret_record_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecord\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret, ret_record\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py:761\u001b[0m, in \u001b[0;36mApp._post_record\u001b[0;34m(self, ret_record_args, error, cost, perf, calls)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedback_mode \u001b[38;5;129;01min\u001b[39;00m [FeedbackMode\u001b[38;5;241m.\u001b[39mDEFERRED,\n\u001b[1;32m    756\u001b[0m                                 FeedbackMode\u001b[38;5;241m.\u001b[39mWITH_APP_THREAD]:\n\u001b[1;32m    757\u001b[0m         TP()\u001b[38;5;241m.\u001b[39mrunlater(\n\u001b[1;32m    758\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error, record\u001b[38;5;241m=\u001b[39mret_record, error\u001b[38;5;241m=\u001b[39merror\n\u001b[1;32m    759\u001b[0m         )\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedback_mode \u001b[38;5;241m==\u001b[39m FeedbackMode\u001b[38;5;241m.\u001b[39mWITH_APP:\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_record(record\u001b[38;5;241m=\u001b[39mret_record)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py:682\u001b[0m, in \u001b[0;36mApp.with_record\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     bindings \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    680\u001b[0m     main_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_input(func, sig, bindings)\n\u001b[0;32m--> 682\u001b[0m     ret, cost \u001b[38;5;241m=\u001b[39m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     main_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_output(func, sig, bindings, ret)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py:331\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[0;34m(thunk, with_openai, with_hugs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[1;32m    322\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m    323\u001b[0m     with_openai: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    324\u001b[0m     with_hugs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_hugs\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py:274\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[0;34m(thunk, with_openai, with_hugs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ApiKeyError:\n\u001b[1;32m    269\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    270\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingface API keys are not set. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWill not track usage.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py:426\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[0;34m(thunk, with_endpoints)\u001b[0m\n\u001b[1;32m    423\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m result: T \u001b[38;5;241m=\u001b[39m \u001b[43mthunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/app.py:683\u001b[0m, in \u001b[0;36mApp.with_record.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m     bindings \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    680\u001b[0m     main_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_input(func, sig, bindings)\n\u001b[1;32m    682\u001b[0m     ret, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m--> 683\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    686\u001b[0m     main_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_output(func, sig, bindings, ret)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py:1036\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1036\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1045\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1046\u001b[0m         )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py:891\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    889\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py:369\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    368\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    373\u001b[0m     )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py:341\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    340\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    344\u001b[0m     )\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/tools/base.py:511\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    518\u001b[0m )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py:1036\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1036\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1045\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1046\u001b[0m         )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py:833\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    830\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 833\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/agents/agent.py:456\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    455\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 456\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py:256\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:846\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    if is_root_call:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m        self.on_add_record(record, func, sig, bindings, cost)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/instruments.py:796\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    # TODO: ROOTLESS\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    # If this is a root call (first instrumented method), also track\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03m    else:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m     rets \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    413\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    414\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py:310\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    309\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    312\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    314\u001b[0m ]\n\u001b[1;32m    315\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py:300\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 300\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m         )\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/base.py:447\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py:340\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 340\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py:279\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/langchain/chat_models/openai.py:277\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py:702\u001b[0m, in \u001b[0;36mEndpoint.wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling wrapped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# Get the result of the wrapped function:\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m response: Any \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m bindings \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# Get all of the callback classes suitable for handling this call.\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# Note that we stored this in the INSTRUMENT attribute of the\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# wrapper method.\u001b[39;00m\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/qual-llm/qual/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-IgcXK***************************************Pjqz. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "truchain_qa_vectordb = TruChain(conversational_agent, \n",
    "    app_id='Knit_AI_Workshop',\n",
    "    feedbacks=[f_lang_match],\n",
    "    tags = \"prototype\")\n",
    "\n",
    "# Instrumented chain can operate like the original:\n",
    "prompt_input=f\"{prompt} What are the most common reasons for cancelling a streaming subscription for people who pay for the streaming service themselves? Plot a chart.\"\n",
    "\n",
    "llm_response = truchain_qa_vectordb(prompt_input)\n",
    "\n",
    "display(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
